
---
title: "02-Decision_Trees_Intro"
output:
  html_document:
    toc: true
---


```{r}
%md
# Decision Trees
### Not focused on best way to perform modelling using Decision Trees or Random Forest
#### Instead focus on intro and getting you started and using Decision Trees & Random Forest
```


```{r}
%md
## Who has used modelling in the past?
### Slack Poll
```


```{r}
library("caret")
library("tidyverse")
data(diamonds)
model <- lm(price ~ ., diamonds)
p <- predict(model, diamonds)
```


```{r}
head(diamonds)
```


```{r}
summary(model)
```


```{r}
summary(p)
```


```{r}
%md
## Decision Trees
* Tree-based models 
 - Non-parametric algorithms 
 - Partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. 
* Predictions are obtained by fitting a simpler model (e.g., a constant like the average response value) in each region. 
* Such divide-and-conquer methods can produce simple rules that are easy to interpret and visualize with tree diagrams. 

```


```{r}
%md
- One methodology for constructing Decision Trees is CART the classification and regression tree (CART) algorithm
  - Partition training data into homogeneous subgroups (nodes)
and then fits a simple constant in each node
- Nodes are formed recursively using binary partitions formed by asking simple yes-or-no questions about each feature (e.g., is age < 18?). 
 - This is done a number of times until a suitable stopping criteria is satisfied (e.g., a maximum depth of the tree is reached). 
- Then the model predicts the output based on 
 - (1) the average response values for all observations that fall in that subgroup (regression problem), or 
 - (2) the class that has majority representation (classification problem). 

```


```{r}
%md

### Example of Decision Tree

![DCT1.jpg](https://cdn-images-1.medium.com/max/824/0*J2l5dvJ2jqRwGDfG.png)


```


```{r}
%md

![DCT2.jpg](https://cdn-images-1.medium.com/max/688/0*pb-1ufHK-OmR8k7r.png)
```


```{r}
library(rpart)
model <- rpart(Species ~., data = iris)
par(xpd = NA) # otherwise on some devices the text is clipped
plot(model)
text(model, digits = 3)
```


```{r}
dim(iris)
```


```{r}
print(model, digits = 2)
```


```{r}
%md
## Decision Trees: Partitioning

- CART uses binary recursive partitioning (it’s recursive because each split or rule depends on the splits above it). 
- The objective at each node is to find the “best” feature to partition the remaining data into one of two regions (R1 and R2) such that the overall error between the actual response (yi) and the predicted constant (ci) is minimized.
- Having found the best feature/split combination, the data are partitioned into two regions and the splitting process is repeated on each of the two regions (hence the name binary recursive partitioning). 

```


### Decision Trees: Deep
- Early stopping
  - Restrict the tree depth to a certain level or 
 - Restrict the mini number of observations allowed in any terminal node
- Pruning
  - grow a very large, complex tree and then prune it back to find an optimal subtree

```{r}
# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
#https://rdrr.io/cran/mlbench/man/PimaIndiansDiabetes.html
```


```{r}
# Split the data into training and test set
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
```


```{r}
# Build the model
set.seed(123)
model1 <- rpart(diabetes ~., data = train.data, method = "class")
# Plot the trees
par(xpd = NA) # Avoid clipping the text in some device
plot(model1)
text(model1, digits = 3)
```


```{r}
# Make predictions on the test data
predicted.classes <- model1 %>% 
  predict(test.data, type = "class")
head(predicted.classes)
```


```{r}
# Compute model accuracy rate on test data
mean(predicted.classes == test.data$diabetes)
```


```{r}
%md
#### Pruning the tree
```


```{r}
?train
```


```{r}
# Fit the model on the training set
set.seed(123)
model2 <- train(
  diabetes ~., data = train.data, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Plot model accuracy vs different values of
# cp (complexity parameter)
plot(model2)
```


```{r}
# Print the best tuning parameter cp that
# maximizes the model accuracy
model2$bestTune
```


```{r}
# Plot the final tree model
par(xpd = NA) # Avoid clipping the text in some device
plot(model2$finalModel)
text(model2$finalModel,  digits = 3)
```


```{r}
# Decision rules in the model
model2$finalModel
```


```{r}
# Make predictions on the test data
predicted.classes <- model2 %>% predict(test.data)
# Compute model accuracy rate on test data
mean(predicted.classes == test.data$diabetes)
```


```{r}
%md
## Take aways: 
- Simple and easy to implement (no preprocessing required and categorical variables handled, handle missing values)
- Not great on predictive accuracy
- Deep trees have high variance / Shallow Trees bias.

## More technical: 
- A problem with decision trees like CART is that they are greedy. 
- Even with Bagging, the decision trees can have a lot of structural similarities and in turn have high correlation in their predictions.
```


```{r}
%md
## Another way to think about Decision Trees
- Great advantage of decision trees: make a complex decision simpler by breaking it down into smaller, simpler decisions using a divide-and-conquer strategy. 
- Identify a set of if-else conditions that split the data according to the value of the features.
```


```{r}
%md
# Before jumping to the next section, let's try out few examples
```

